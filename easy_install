hack/install-etcd.sh
export PATH="/home/ubuntu/go/src/kubernetes/kubernetes/third_party/etcd:${PATH}"
hack/local-up-cluster.sh
https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/
CLOUD_PROVIDER=aws HOSTNAME_OVERRIDE=ip-10-63-10-66.us-west-2.compute.internal hack/local-up-cluster.sh
I0924 18:57:18.597038   13405 event.go:255] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"my-service", UID:"7d657d39-ec02-4a40-bc53-e7222005386f", APIVersion:"v1", ResourceVersion:"387", FieldPath:""}): type: 'Warning' reason: 'SyncLoadBalancerFailed' Error syncing load balancer: failed to ensure load balancer: could not find any suitable subnets for creating the ELB
this is what goes to the events in describe
cluster/kubectl.sh get nodes -o json | jq .items[].spec.taints
annotating resources https://github.com/kubernetes/kubernetes/issues/29298
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2019-09-24T21:05:04Z"
  finalizers:
  - service.kubernetes.io/load-balancer-cleanup
  labels:
    app.kubernetes.io/name: load-balancer-example
  name: my-service
  namespace: default
  resourceVersion: "405"
  selfLink: /api/v1/namespaces/default/services/my-service
  uid: ab275a40-bd7b-4c93-8caa-4df6074dfa06
  annotations:
          service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
spec:
  clusterIP: 10.0.0.253
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 31129
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app.kubernetes.io/name: load-balancer-example
  sessionAffinity: None
  type: LoadBalancer
status:
  loadBalancer: {}
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app.kubernetes.io/name":"load-balancer-example"},"name":"hello-world","namespace":"default"},"spec":{"replicas":5,"selector":{"matchLabels":{"app.kubernetes.io/name":"load-balancer-example"}},"template":{"metadata":{"labels":{"app.kubernetes.io/name":"load-balancer-example"}},"spec":{"containers":[{"image":"gcr.io/google-samples/node-hello:1.0","name":"hello-world","ports":[{"containerPort":8080}]}]}}}}
  creationTimestamp: "2019-09-24T21:19:27Z"
  generation: 1
  labels:
    app.kubernetes.io/name: load-balancer-example
  name: hello-world
  namespace: default
  resourceVersion: "367"
  selfLink: /apis/apps/v1/namespaces/default/deployments/hello-world
  uid: a3ad771f-08b8-4053-b846-499aca5e9b46
spec:
  progressDeadlineSeconds: 600
  replicas: 5
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: load-balancer-example
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app.kubernetes.io/name: load-balancer-example
    spec:
      containers:
      - image: gcr.io/google-samples/node-hello:1.0
        imagePullPolicy: IfNotPresent
        name: hello-world
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 5
  conditions:
  - lastTransitionTime: "2019-09-24T21:19:31Z"
    lastUpdateTime: "2019-09-24T21:19:31Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2019-09-24T21:19:27Z"
    lastUpdateTime: "2019-09-24T21:19:31Z"
    message: ReplicaSet "hello-world-f9b447754" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 5
  replicas: 5
  updatedReplicas: 5
  kubelet_node_status.go:94] Unable to register node "ip-10-63-130-255.us-west-2.compute.internal" with API server: nodes "ip-10-63-130-255.us-west-2.compute.internal" is forbidden: node "ip-10-63-130-255" is not allowed to modify node "ip-10-63-130-255.us-west-2.compute.internal"
https://github.com/kubernetes/kubeadm/issues/584
